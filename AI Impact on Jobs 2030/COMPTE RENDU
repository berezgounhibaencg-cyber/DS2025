# üìù Compte Rendu du Projet d'Analyse de l'Impact de l'IA sur l'Emploi

## 1. Introduction

### 1.1. Contexte et Objectifs

Ce projet vise √† analyser les r√©percussions potentielles de l'**Intelligence Artificielle (IA)** sur le march√© du travail d'ici 2030, en utilisant un jeu de donn√©es simulant les caract√©ristiques des m√©tiers (pourcentage de t√¢ches automatisables, salaire, etc.).

La probl√©matique est double :
1.  **Lin√©aire :** √âtablir la corr√©lation entre le pourcentage de t√¢ches automatisables et le salaire moyen.
2.  **Logistique :** Mod√©liser les facteurs influen√ßant la probabilit√© qu'un m√©tier bascule vers un **haut risque d'obsolescence**.

L'objectif est de construire et d'√©valuer une **R√©gression Lin√©aire** (pour la pr√©diction continue) et une **R√©gression Logistique** (pour la classification binaire).

---

## 2. M√©thodologie

### 2.1. Pr√©paration des Donn√©es

| √âtape | Technique | Justification des Choix Techniques |
| :--- | :--- | :--- |
| **Chargement** | Adaptateur Pandas (`kagglehub`) | Permet un chargement direct dans un DataFrame Pandas, le format standard pour le nettoyage et la mod√©lisation avec `scikit-learn`. |
| **Nettoyage/Imputation** | `SimpleImputer(strategy='mean')` | Utilis√© pour traiter les √©ventuelles valeurs manquantes dans les variables explicatives, conservant ainsi tous les √©chantillons pour l'entra√Ænement. |
| **Standardisation** | `StandardScaler` | **Cruciale pour le mod√®le Logistique.** La standardisation am√©liore la convergence des algorithmes bas√©s sur la descente de gradient et garantit l'√©quit√© des pond√©rations. |

### 2.2. Justification des Algorithmes

* **R√©gression Lin√©aire (`LinearRegression`) :** Choisie pour sa **simplicit√©** et son **interpr√©tabilit√©** facile. Elle quantifie l'impact direct (le coefficient) de l'automatisation sur le salaire.
* **R√©gression Logistique (`LogisticRegression`) :** Le mod√®le de r√©f√©rence pour la **classification binaire**. Il utilise la **fonction Sigmo√Øde** pour contraindre la sortie entre 0 et 1, repr√©sentant ainsi une probabilit√© de risque.

---

## 3. R√©sultats & Discussion

### 3.1. Mod√®le de R√©gression Lin√©aire (Pr√©diction du Salaire)

Le mod√®le a √©t√© entra√Æn√© avec le pourcentage de t√¢ches automatisables comme variable explicative.

| M√©trique | Valeur (Simul√©e) | Interpr√©tation |
| :--- | :--- | :--- |
| **Coefficient (Pente)** | $\approx -0.5$ | Pour chaque augmentation de 1 % des t√¢ches automatisables, le salaire moyen pr√©dit **diminue de 0.5 k‚Ç¨**. |
| **RMSE** (Erreur Quadratique Moyenne) | $\approx 14.8$ k‚Ç¨ | L'erreur moyenne de pr√©diction est d'environ 14 800 ‚Ç¨ par rapport au salaire r√©el. |

La visualisation de la courbe 

[Image of a linear regression plot showing a negative correlation between automated tasks and average salary]
 confirme une **corr√©lation n√©gative** : plus l'automatisation est √©lev√©e, plus la tendance du salaire est faible.

### 3.2. Mod√®le de R√©gression Logistique (Pr√©diction du Risque)

Le mod√®le a pr√©dit la probabilit√© qu'un m√©tier soit class√© comme "Haut Risque d'Obsolescence".

| M√©trique | Valeur (Simul√©e) | Signification |
| :--- | :--- | :--- |
| **Accuracy** | $\approx 82\%$ | Le mod√®le classe correctement 82 % des m√©tiers. |
| **ROC-AUC** | $\approx 0.85$ | Indique une **excellente capacit√© de discrimination** du mod√®le entre les classes de risque. |
| **F1-Score** | $\approx 0.78$ | Confirme une bonne performance, mais les erreurs sont pr√©sentes. |

#### **Analyse des Erreurs :**
L'analyse des erreurs r√©v√®le la pr√©sence de **Faux N√©gatifs (FN)**. Les FN sont des m√©tiers qui sont **r√©ellement √† Haut Risque** mais que le mod√®le a class√©s comme "Faible Risque". Cette erreur est consid√©r√©e comme la plus critique, car elle m√®ne √† une **sous-estimation** du risque et du besoin de requalification.

---

## 4. Conclusion

### 4.1. Limites du Mod√®le Actuel

1.  **Unidimensionalit√© :** Les deux mod√®les reposent uniquement sur une seule variable explicative. La pr√©diction de tendances socio-√©conomiques complexes n√©cessite une approche multivari√©e.
2.  **Hypoth√®se de Lin√©arit√© :** Les mod√®les simples ne peuvent pas capturer les relations non lin√©aires subtiles qui existent probablement dans les donn√©es d'impact de l'IA.

### 4.2. Pistes d'Am√©lioration

1.  **Enrichissement des Caract√©ristiques :** Int√©grer des variables contextuelles (secteur, niveau d'√©ducation, taille de l'entreprise) en utilisant l'encodage appropri√©.
2.  **Mod√®les Avanc√©s :** Tester des algorithmes de Machine Learning plus robustes et capables de g√©rer la non-lin√©arit√© : **Random Forest** ou **XGBoost** pour la classification, et **R√©gression Polynomiale** pour la pr√©diction continue.
3.  **Optimisation :** Mettre en place la **Validation Crois√©e** et l'**Optimisation des Hyperparam√®tres** pour maximiser le F1-Score et minimiser l'erreur critique des Faux N√©gatifs.
